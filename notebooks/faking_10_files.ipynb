{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faking 10 Files\n",
    "\n",
    "1. Always build with 10 files output chunks\n",
    "1. If the actual number is less than 10, then have zero items in some of the chunks\n",
    "1. If more than 10 files, try to combine them with uproot5 and dask or similar."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faker\n",
    "\n",
    "Initialize with the proper number of files we'll eventually generate. Mostly copied from the `with_histo` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import awkward as ak\n",
    "import dask_awkward as dak\n",
    "import random\n",
    "import dask\n",
    "\n",
    "def make_input_layer(name: str, inputs: List[str], npartitions=10):\n",
    "    '''Create an AwkwardInput layer with `inputs` chunks. Each chunk\n",
    "    has some random numbers in it (100 of them). This will always generate\n",
    "    10 partitions. If `len(inputs)` is less than 10, then some of the partitions\n",
    "    will be empty. If `len(inputs)` is greater than 10, then we will\n",
    "    combine some of them.\n",
    "\n",
    "    Args:\n",
    "        name (str): Name of the input layer (for the graph)\n",
    "        inputs (List[str]): Names of each partition\n",
    "\n",
    "    Returns:\n",
    "        AwkwardInputLayer: Input Layer\n",
    "    '''\n",
    "    def generate_data(block):\n",
    "        '''Generate the data for a particular partition. We have the job of\n",
    "        figuring out how many files per partition.\n",
    "\n",
    "        Args:\n",
    "            block (int): The block number\n",
    "        '''\n",
    "        if len(inputs) <= npartitions:\n",
    "            if block < len(inputs):\n",
    "                return generate_partition_data(inputs[block])\n",
    "            else:\n",
    "                return ak.from_iter([])\n",
    "        else:\n",
    "            num_per = int(len(inputs) / npartitions)\n",
    "            if block < (len(inputs) - num_per*npartitions):\n",
    "                num_per += 1\n",
    "            \n",
    "            block_index = num_per * block\n",
    "            data = [generate_partition_data(inputs[block_index + i]) for i in range(0, num_per)]\n",
    "            return ak.concatenate(data)\n",
    "\n",
    "    def generate_partition_data(block):\n",
    "        '''Generate the partition data for a single block.\n",
    "\n",
    "        Args:\n",
    "            block (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        '''\n",
    "        print(f'In generate_data: {block}')\n",
    "        return ak.from_iter([random.uniform(0, 10) for i in range(0, 100)])\n",
    "\n",
    "    # Build the metadata for this array we will be returning. Each partition\n",
    "    # will be of this form.\n",
    "    sample_array = ak.from_iter([1.0, 2.1, 3.2, 4.3, 5.4])\n",
    "    metadata = dak.core.typetracer_array(sample_array)\n",
    "\n",
    "    # Next, create the input layer that will be used to generate the data.\n",
    "    # Always setup 10 partitions\n",
    "    dsk = dak.layers.AwkwardInputLayer(\n",
    "            name=name,\n",
    "            columns=None,\n",
    "            inputs=list(range(0, npartitions)),\n",
    "            io_func=generate_data,\n",
    "            meta=metadata,\n",
    "            behavior=None,\n",
    "        )\n",
    "\n",
    "    return dsk\n",
    "\n",
    "def generate_sx_daq(query: str, inputs: List[str] = ['0', '1'], n_partitions = 10) -> dak.Array:\n",
    "    name = 'unique-name'\n",
    "    input_layer = make_input_layer(name, inputs, npartitions=n_partitions)\n",
    "\n",
    "    # Create the high level graph that will hold all of this, and the actual array object\n",
    "    hlg = dask.highlevelgraph.HighLevelGraph.from_collections(name, input_layer)\n",
    "    ar = dak.core.new_array_object(hlg, name, meta=input_layer._meta, npartitions=n_partitions)\n",
    "\n",
    "    return ar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a histogram with exactly 10 partitions\n",
    "\n",
    "This is the easy case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In generate_data: b_0\n",
      "In generate_data: b_1\n",
      "In generate_data: b_2\n",
      "In generate_data: b_3\n",
      "In generate_data: b_4\n",
      "In generate_data: b_5\n",
      "In generate_data: b_6\n",
      "In generate_data: b_7\n",
      "In generate_data: b_8\n",
      "In generate_data: b_9\n"
     ]
    }
   ],
   "source": [
    "import dask_histogram as dh\n",
    "import mplhep as hep\n",
    "\n",
    "x = generate_sx_daq(\"(query)\", inputs=[f'b_{i}' for i in range(0, 10)])\n",
    "h = dh.factory(x, axes=(dh.axis.Regular(20, 0, 10),))\n",
    "r = h.compute()\n",
    "#_ = hep.histplot(r)\n",
    "\n",
    "assert r.sum() == 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a histogram with 5 partitions\n",
    "\n",
    "So 5 of them should be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In generate_data: b_0In generate_data: b_1\n",
      "In generate_data: b_2\n",
      "In generate_data: b_3\n",
      "In generate_data: b_4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dask_histogram as dh\n",
    "import mplhep as hep\n",
    "\n",
    "x = generate_sx_daq(\"(query)\", inputs=[f'b_{i}' for i in range(0, 5)])\n",
    "h = dh.factory(x, axes=(dh.axis.Regular(20, 0, 10),))\n",
    "r = h.compute()\n",
    "#_ = hep.histplot(r)\n",
    "\n",
    "assert r.sum() == 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a histogram with 15 partitions\n",
    "\n",
    "This means that some them should have more than one partition in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In generate_data: b_0In generate_data: b_2\n",
      "In generate_data: b_3\n",
      "In generate_data: b_4\n",
      "In generate_data: b_5\n",
      "In generate_data: b_6\n",
      "In generate_data: b_7\n",
      "In generate_data: b_8\n",
      "In generate_data: b_9\n",
      "In generate_data: b_5\n",
      "In generate_data: b_6\n",
      "In generate_data: b_7\n",
      "In generate_data: b_8\n",
      "In generate_data: b_9\n",
      "\n",
      "In generate_data: b_1\n"
     ]
    }
   ],
   "source": [
    "import dask_histogram as dh\n",
    "import mplhep as hep\n",
    "\n",
    "x = generate_sx_daq(\"(query)\", inputs=[f'b_{i}' for i in range(0, 15)])\n",
    "h = dh.factory(x, axes=(dh.axis.Regular(20, 0, 10),))\n",
    "r = h.compute()\n",
    "\n",
    "assert r.sum() == 1500"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
