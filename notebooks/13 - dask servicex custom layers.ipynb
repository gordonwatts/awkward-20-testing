{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DASK to ServiceX\n",
    "\n",
    "In this demo we'll take advantage of DASK and ServiceX. This work is driven by the fact that `AwkwardInputLayer` seems like it will not take tasks as inputs. So we need to move onto something else.\n",
    "\n",
    "## Assumptions:\n",
    "\n",
    "* We don't start anything until we know the number of files that SX will produce\n",
    "* We are ok with some files failing coming out of SX\n",
    "* We are going to do one partition per file\n",
    "* When we start we don't necessarily know all the files produced.\n",
    "\n",
    "## Design Outline\n",
    "\n",
    "* A single `dask` task/layer that has a single output per partition. The output is just a string.\n",
    "* The `AwkwardInputLayer` that has looks at the task input and loads that data from `minio`\n",
    "\n",
    "This version of things will work only for local files - once this works we can move it to a SX prototype.\n",
    "\n",
    "This was written before any code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_awkward as dak\n",
    "import awkward as ak\n",
    "import dask\n",
    "import uproot\n",
    "\n",
    "from dask.highlevelgraph import Layer, HighLevelGraph\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from typing import AbstractSet\n",
    "\n",
    "import logging\n",
    "\n",
    "# Make debugging a little easier...\n",
    "cluster = LocalCluster(processes=False)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `uproot.dask` hack way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `awkward.Form` file form\n",
    "\n",
    "We need the form from the schema to prevent us from having to open files that do not yet exist in hour hack. Eventually we'll have to build this from the schema we know exists from the `func_adl` query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RecordForm([ListOffsetForm('i64', NumpyForm('float64'))], ['JetPt']),\n",
       " <Array-typetracer [...] type='## * {JetPt: var * float64}'>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_filename = \"0fc6e51a5ea6dea107c195591d20a1b2-15.26710677._000019.pool.root.1\"\n",
    "with uproot.open(dummy_filename) as file:\n",
    "    file_form = file['treeme'].arrays().layout.form\n",
    "    metadata = dak.core.typetracer_array(file['treeme'].arrays())\n",
    "\n",
    "file_form, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>[[102, 63.1],\n",
       " [69.3, 50.2, 35.7],\n",
       " [110, 34.5, 33.1],\n",
       " [133, 101],\n",
       " [126, 55.7, 38],\n",
       " [81.1, 54.4, 35.3, 32],\n",
       " [70.6, 70.5],\n",
       " [131, 96.9, 36.6, 30.5],\n",
       " [45.1, 37.6],\n",
       " [97.2, 76.4],\n",
       " ...,\n",
       " [134, 115, 30.4],\n",
       " [142, 128, 32.5],\n",
       " [152, 109, 34.1],\n",
       " [116, 93.6, 34.2, 31.9],\n",
       " [64, 50.8, 39.5],\n",
       " [114, 92.4, 33.6, 32],\n",
       " [121, 109, 63.1],\n",
       " [114, 46.6, 44.4, 32.9],\n",
       " [153, 90.5, 54]]\n",
       "---------------------------\n",
       "type: 50000 * var * float64</pre>"
      ],
      "text/plain": [
       "<Array [[102, 63.1], [...], ..., [153, 90.5, 54]] type='50000 * var * float64'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ar = uproot.dask({dummy_filename: \"treeme\"}, open_files=False, known_base_form=file_form)\n",
    "test_ar.JetPt.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blockwise Approach\n",
    "\n",
    "Could we start a blockwise approach on its own?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SXLayerBW(Layer):\n",
    "    '''Outputs are just the names of the files that we want to open downstream with uproot'''\n",
    "    def __init__(self, name, n_files):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.dependencies = dict()\n",
    "        self.tasks = {\n",
    "            \"output_0\": (lambda: self.get_file(f\"output_0\"),)\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.tasks[key]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.tasks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tasks)\n",
    "\n",
    "    def is_materialized(self):\n",
    "        return False\n",
    "    \n",
    "    def get_output_keys(self) -> AbstractSet[str | bytes | int | float]:\n",
    "        return set(self.tasks.keys())\n",
    "    \n",
    "    def get_file(self, name):\n",
    "        '''Return the info that is needed by uproot to actually open the file'''\n",
    "        print(f\"Returning info for file {name}: {dummy_filename}\")\n",
    "        return (dummy_filename, 'treeme')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the layer that will load files from the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class URLoaderLayer(Layer):\n",
    "    def __init__(self, name, sx_layer_name, output_name, n_files):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.dependencies = {name: sx_layer_name}\n",
    "        self.tasks = {\n",
    "            (name, i): (lambda f_name: self.get_data(f_name), f'output_{i}')\n",
    "            for i in range(n_files)\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.tasks[key]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.tasks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tasks)\n",
    "\n",
    "    def is_materialized(self):\n",
    "        return False\n",
    "    \n",
    "    def get_output_keys(self) -> AbstractSet[str | bytes | int | float]:\n",
    "        return set(self.tasks.keys())\n",
    "    \n",
    "    def get_data(self, name):\n",
    "        '''Return the info that is needed by uproot to actually open the file'''\n",
    "        # TODO: This is swallowed unless we use a dask `LocalCluster`.\n",
    "        logging.warning(f\"Returning info for file {name}\")\n",
    "        with uproot.open(name[0]) as file:\n",
    "            return file[name[1]].arrays()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok - lets build up the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the layers\n",
    "\n",
    "sx_layer = SXLayerBW(\"sx_fetcher\", 1)\n",
    "loader_layer = URLoaderLayer(\"uproot_loader\", \"sx_fetcher\", \"output\", 1)\n",
    "\n",
    "# Now, the high level layer...\n",
    "hlg = HighLevelGraph(\n",
    "    layers={sx_layer.name: sx_layer, loader_layer.name: loader_layer},\n",
    "    dependencies={loader_layer.name: {sx_layer.name}, sx_layer.name: set()},\n",
    ")\n",
    "\n",
    "# And finally the array...\n",
    "ar = dak.core.new_array_object(hlg, \"uproot_loader\", meta=metadata, npartitions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Returning info for file ('0fc6e51a5ea6dea107c195591d20a1b2-15.26710677._000019.pool.root.1', 'treeme')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning info for file output_0: 0fc6e51a5ea6dea107c195591d20a1b2-15.26710677._000019.pool.root.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>[[102],\n",
       " [],\n",
       " [110],\n",
       " [133, 101],\n",
       " [126],\n",
       " [],\n",
       " [],\n",
       " [131],\n",
       " [],\n",
       " [],\n",
       " ...,\n",
       " [134, 115],\n",
       " [142, 128],\n",
       " [152, 109],\n",
       " [116],\n",
       " [],\n",
       " [114],\n",
       " [121, 109],\n",
       " [114],\n",
       " [153]]\n",
       "---------------------------\n",
       "type: 50000 * var * float64</pre>"
      ],
      "text/plain": [
       "<Array [[102], [], [110], ..., [114], [153]] type='50000 * var * float64'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-24 20:29:12,534 - distributed.scheduler - WARNING - Worker failed to heartbeat within 300 seconds. Closing: <WorkerState 'inproc://192.168.1.16/27104/4', name: 0, status: running, memory: 0, processing: 0>\n",
      "2024-02-24 20:29:12,546 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'inproc://192.168.1.16/27104/4'.\n",
      "2024-02-24 20:29:12,548 - distributed.worker - ERROR - Scheduler was unaware of this worker 'inproc://192.168.1.16/27104/4'. Shutting down.\n"
     ]
    }
   ],
   "source": [
    "ar.JetPt[ar.JetPt > 100.0].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SX Support\n",
    "\n",
    "The code below belongs in the `sx-awk` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the ServiceX layer. It is responsible for all communication with ServiceX, and finding the files (and URL's) from `minio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Set\n",
    "from typing import AbstractSet, Any, Dict, Iterator, KeysView, List\n",
    "import logging\n",
    "\n",
    "class SXLayer(Layer):\n",
    "    def __init__(self, sx_query_guid):\n",
    "        super().__init__()\n",
    "        self._query_guid = sx_query_guid\n",
    "\n",
    "        # Create a task that will be executed when the layer is computed,\n",
    "        # and will fetch the list of files from SX.\n",
    "        k = f\"SX-query-{self._query_guid}\"\n",
    "        self._tasks: Dict[str, Any] = {k: dask.delayed(self._fetch_files, name=k)}\n",
    "\n",
    "    def _fetch_files(self) -> str:\n",
    "        # This is where the actual fetching of the files from SX would happen.\n",
    "        # For now, just return a list of files.\n",
    "        logging.warn(\"Returning a file\")\n",
    "        return \"0fc6e51a5ea6dea107c195591d20a1b2-15.26710677._000019.pool.root.1\"\n",
    "\n",
    "    def __getitem__(self, __key) -> Any:\n",
    "        return self._tasks[str(__key)]\n",
    "    \n",
    "    def keys(self):\n",
    "        return self._tasks.keys()\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self._tasks)\n",
    "    \n",
    "    def get_output_keys(self) -> AbstractSet[str]:\n",
    "        return {f\"SX-query-{self._query_guid}\"}\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._tasks)\n",
    "    \n",
    "    def is_materialized(self) -> bool:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = SXLayer(\"182382781\")\n",
    "hlg = HighLevelGraph(\n",
    "    layers={\"l1\": l},\n",
    "    dependencies={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = client.compute(hlg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `uproot.dask` way\n",
    "\n",
    "This is a very simple call - here for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"0fc6e51a5ea6dea107c195591d20a1b2-15.26710677._000019.pool.root.1\"\n",
    "ar = uproot.dask({filename: \"treeme\"}, open_files=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = ar.JetPt * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lets look at the layers/etc. for this for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = pt.__dask_graph__()\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the input layer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_uproot_key = [k for k in graph.layers.keys() if k.startswith(\"from\")][0]\n",
    "print(f\"From uproot key: {from_uproot_key}\")\n",
    "from_uproot_first_output = list(graph.layers[from_uproot_key].keys())[0]\n",
    "print(f\"From uproot first output: {from_uproot_first_output}\")\n",
    "print(f\"The function that gets executed and the arguments: {graph.layers[from_uproot_key][from_uproot_first_output]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the `('0fc6e51a5ea6dea107c195591d20a1b2-15.26710677._000019.pool.root.1', 'treeme', 0, 1, False)` to be the argument/output from a previous run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Demo\n",
    "\n",
    "Used AI to come up with this (minus a few syntax errors and missing the `Client.get`). This shows how to build everything from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Set\n",
    "import typing\n",
    "\n",
    "\n",
    "class CustomLayer(Layer):\n",
    "    def __init__(self, name, dependencies, tasks):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.dependencies = dependencies\n",
    "        self.tasks = tasks\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.tasks[key]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.tasks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tasks)\n",
    "\n",
    "    def is_materialized(self):\n",
    "        return False\n",
    "    \n",
    "    def get_output_keys(self) -> AbstractSet[str | bytes | int | float]:\n",
    "        return set(self.tasks.keys())\n",
    "\n",
    "# Define the tasks for each layer\n",
    "tasks1 = {\"output1\": (lambda: \"result1\", ), \"output2\": (lambda: \"result2\", )}\n",
    "tasks2 = {\"output3\": (lambda x: x + \" processed\", \"output1\"), \"output4\": (lambda x: x + \" processed\", \"output2\")}\n",
    "\n",
    "# Create the layers\n",
    "layer1 = CustomLayer(\"layer1\", [], tasks1)\n",
    "layer2 = CustomLayer(\"layer2\", [\"layer1\"], tasks2)\n",
    "\n",
    "# Create the high level graph\n",
    "hlg = HighLevelGraph(\n",
    "    layers={layer1.name: layer1, layer2.name: layer2},\n",
    "    dependencies={layer2.name: {layer1.name}, layer1.name: set()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get(hlg, [\"output4\", \"output3\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
