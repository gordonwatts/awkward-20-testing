{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DASK to ServiceX\n",
    "\n",
    "In this demo we'll take advantage of DASK and ServiceX. This work is driven by the fact that `AwkwardInputLayer` seems like it will not take tasks as inputs. So we need to move onto something else.\n",
    "\n",
    "## Assumptions:\n",
    "\n",
    "* We don't start anything until we know the number of files that SX will produce. Thus we know the number of partitions up front.\n",
    "* We are ok with some files failing coming out of SX\n",
    "* We are going to do one partition per file\n",
    "* When we start we don't know all the _names_ of the files produced.\n",
    "* We have to fetch them from minio, and that doesn't put them in any sort of time order.\n",
    "\n",
    "## Design Outline\n",
    "\n",
    "* A single `dask` task/layer that has a single output per partition. The output is just a string, indicating the file we need to open in uproot.\n",
    "* A single thread polls ServiceX looking for new files to show up, and when they do, it passes them to a `dask` output. A `dask` task then tries to open the file.\n",
    "\n",
    "The problem:\n",
    "* `dask` is built around the idea that any task can be re-run with the exact same results.\n",
    "* In this implementation we are using a `queue`, which means each time you run, you'll get a different answer (yes, we could fix that easily).\n",
    "* But the basic problem ends up being: the thread that inserts the items on the `queue` might be in a different process than the `task` that needs to read the `queue`.\n",
    "* `dask` points this out by refusing to pickle a lock.\n",
    "\n",
    "There is a work around: the single thread can write a file to some shared file system with the file names in order of discovery, and then everyone can read that in. However, it might be that there is a better solution - that feels dirty!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_awkward as dak\n",
    "import awkward as ak\n",
    "import dask\n",
    "import uproot\n",
    "\n",
    "from dask.highlevelgraph import Layer, HighLevelGraph\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from typing import AbstractSet\n",
    "\n",
    "import logging\n",
    "\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "\n",
    "# Make debugging a little easier...\n",
    "cluster = LocalCluster(processes=False)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `uproot.dask` hack way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `awkward.Form` file form\n",
    "\n",
    "We need the form from the schema to prevent us from having to open files that do not yet exist in hour hack. Eventually we'll have to build this from the schema we know exists from the `func_adl` query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RecordForm([ListOffsetForm('i64', NumpyForm('float64'))], ['JetPt']),\n",
       " <Array-typetracer [...] type='## * {JetPt: var * float64}'>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_filename = \"0fc6e51a5ea6dea107c195591d20a1b2-15.26710677._000019.pool.root.1\"\n",
    "with uproot.open(dummy_filename) as file:\n",
    "    file_form = file['treeme'].arrays().layout.form\n",
    "    metadata = dak.core.typetracer_array(file['treeme'].arrays())\n",
    "\n",
    "file_form, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>[[102, 63.1],\n",
       " [69.3, 50.2, 35.7],\n",
       " [110, 34.5, 33.1],\n",
       " [133, 101],\n",
       " [126, 55.7, 38],\n",
       " [81.1, 54.4, 35.3, 32],\n",
       " [70.6, 70.5],\n",
       " [131, 96.9, 36.6, 30.5],\n",
       " [45.1, 37.6],\n",
       " [97.2, 76.4],\n",
       " ...,\n",
       " [134, 115, 30.4],\n",
       " [142, 128, 32.5],\n",
       " [152, 109, 34.1],\n",
       " [116, 93.6, 34.2, 31.9],\n",
       " [64, 50.8, 39.5],\n",
       " [114, 92.4, 33.6, 32],\n",
       " [121, 109, 63.1],\n",
       " [114, 46.6, 44.4, 32.9],\n",
       " [153, 90.5, 54]]\n",
       "---------------------------\n",
       "type: 50000 * var * float64</pre>"
      ],
      "text/plain": [
       "<Array [[102, 63.1], [...], ..., [153, 90.5, 54]] type='50000 * var * float64'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ar = uproot.dask({dummy_filename: \"treeme\"}, open_files=False, known_base_form=file_form)\n",
    "test_ar.JetPt.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blockwise Approach\n",
    "\n",
    "Could we start a blockwise approach on its own?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "class SXLayerBW(Layer):\n",
    "    '''Outputs are just the names of the files that we want to open downstream with uproot'''\n",
    "    def __init__(self, name, n_files):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.dependencies = dict()\n",
    "        manager = multiprocessing.Manager()\n",
    "        self.queue = manager.Queue()\n",
    "        self.tasks = {f\"output_{i}\": (self.get_file, f\"output_{i}\") for i in range(n_files)}\n",
    "        self.polling_thread = threading.Thread(target=self.poll_api)\n",
    "        self.polling_thread.start()\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.tasks[key]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.tasks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tasks)\n",
    "\n",
    "    def is_materialized(self):\n",
    "        return False\n",
    "    \n",
    "    def get_output_keys(self) -> AbstractSet[str | bytes | int | float]:\n",
    "        return set(self.tasks.keys())\n",
    "    \n",
    "    def poll_api(self):\n",
    "        '''Poll the API and fetch the files'''\n",
    "        for i in range(len(self.tasks)):\n",
    "            print(f\"Fetching info for file output_{i}: {dummy_filename}\")\n",
    "            time.sleep(2)  # simulate delay\n",
    "            self.queue.put((dummy_filename, 'treeme'))\n",
    "    \n",
    "    def get_file(self, name):\n",
    "        '''Return the info that is needed by uproot to actually open the file'''\n",
    "        print(f\"Returning info for file {name}\")\n",
    "        return self.queue.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the layer that will load files from the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class URLoaderLayer(Layer):\n",
    "    def __init__(self, name, sx_layer_name, n_files):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.dependencies = {name: sx_layer_name}\n",
    "        self.tasks = {\n",
    "            (name, i): (lambda f_name: self.get_data(f_name), f'output_{i}')\n",
    "            for i in range(n_files)\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.tasks[key]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.tasks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tasks)\n",
    "\n",
    "    def is_materialized(self):\n",
    "        return False\n",
    "    \n",
    "    def get_output_keys(self) -> AbstractSet[str | bytes | int | float]:\n",
    "        return set(self.tasks.keys())\n",
    "    \n",
    "    def get_data(self, name):\n",
    "        '''Return the info that is needed by uproot to actually open the file'''\n",
    "        # TODO: This is swallowed unless we use a dask `LocalCluster`.\n",
    "        logging.warning(f\"Returning info for file {name}\")\n",
    "        with uproot.open(name[0]) as file:\n",
    "            return file[name[1]].arrays()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok - lets build up the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching info for file output_0: 0fc6e51a5ea6dea107c195591d20a1b2-15.26710677._000019.pool.root.1\n"
     ]
    }
   ],
   "source": [
    "# the layers\n",
    "\n",
    "sx_layer = SXLayerBW(\"sx_fetcher\", 1)\n",
    "loader_layer = URLoaderLayer(\"uproot_loader\", \"sx_fetcher\", 1)\n",
    "\n",
    "# Now, the high level layer...\n",
    "hlg = HighLevelGraph(\n",
    "    layers={sx_layer.name: sx_layer, loader_layer.name: loader_layer},\n",
    "    dependencies={loader_layer.name: {sx_layer.name}, sx_layer.name: set()},\n",
    ")\n",
    "\n",
    "# And finally the array...\n",
    "ar = dak.core.new_array_object(hlg, \"uproot_loader\", meta=metadata, npartitions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-24 22:33:45,028 - distributed.protocol.pickle - ERROR - Failed to serialize <ToPickle: HighLevelGraph with 3 layers.\n",
      "<dask.highlevelgraph.HighLevelGraph object at 0x25c91d93390>\n",
      " 0. sx_fetcher\n",
      " 1. uproot_loader\n",
      " 2. getitem-917c0def67e4544c397316e729412f33\n",
      ">.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\gordo\\Code\\iris-hep\\awkward-20-testing\\.venv\\Lib\\site-packages\\distributed\\protocol\\pickle.py\", line 63, in dumps\n",
      "    result = pickle.dumps(x, **dump_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't pickle local object 'unpack_collections.<locals>.repack'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\gordo\\Code\\iris-hep\\awkward-20-testing\\.venv\\Lib\\site-packages\\distributed\\protocol\\pickle.py\", line 68, in dumps\n",
      "    pickler.dump(x)\n",
      "AttributeError: Can't pickle local object 'unpack_collections.<locals>.repack'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\gordo\\Code\\iris-hep\\awkward-20-testing\\.venv\\Lib\\site-packages\\distributed\\protocol\\pickle.py\", line 81, in dumps\n",
      "    result = cloudpickle.dumps(x, **dump_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gordo\\Code\\iris-hep\\awkward-20-testing\\.venv\\Lib\\site-packages\\cloudpickle\\cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"c:\\Users\\gordo\\Code\\iris-hep\\awkward-20-testing\\.venv\\Lib\\site-packages\\cloudpickle\\cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: cannot pickle '_thread.lock' object\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "('Could not serialize object of type HighLevelGraph', '<ToPickle: HighLevelGraph with 3 layers.\\n<dask.highlevelgraph.HighLevelGraph object at 0x25c91d93390>\\n 0. sx_fetcher\\n 1. uproot_loader\\n 2. getitem-917c0def67e4544c397316e729412f33\\n>')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\gordo\\Code\\iris-hep\\awkward-20-testing\\.venv\\Lib\\site-packages\\distributed\\protocol\\pickle.py:63\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(x, buffer_callback, protocol)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 63\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdump_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't pickle local object 'unpack_collections.<locals>.repack'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\gordo\\Code\\iris-hep\\awkward-20-testing\\.venv\\Lib\\site-packages\\distributed\\protocol\\pickle.py:68\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(x, buffer_callback, protocol)\u001b[0m\n\u001b[0;32m     67\u001b[0m buffers\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m---> 68\u001b[0m \u001b[43mpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m result \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't pickle local object 'unpack_collections.<locals>.repack'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\gordo\\Code\\iris-hep\\awkward-20-testing\\.venv\\Lib\\site-packages\\distributed\\protocol\\serialize.py:353\u001b[0m, in \u001b[0;36mserialize\u001b[1;34m(x, serializers, on_error, context, iterate_collection)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     header, frames \u001b[38;5;241m=\u001b[39m \u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m wants_context \u001b[38;5;28;01melse\u001b[39;00m dumps(x)\n\u001b[0;32m    354\u001b[0m     header[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m name\n",
      "File \u001b[1;32mc:\\Users\\gordo\\Code\\iris-hep\\awkward-20-testing\\.venv\\Lib\\site-packages\\distributed\\protocol\\serialize.py:76\u001b[0m, in \u001b[0;36mpickle_dumps\u001b[1;34m(x, context)\u001b[0m\n\u001b[0;32m     74\u001b[0m     writeable\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadonly)\n\u001b[1;32m---> 76\u001b[0m frames[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpickle-protocol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m header \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwriteable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtuple\u001b[39m(writeable),\n\u001b[0;32m     84\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\gordo\\Code\\iris-hep\\awkward-20-testing\\.venv\\Lib\\site-packages\\distributed\\protocol\\pickle.py:81\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(x, buffer_callback, protocol)\u001b[0m\n\u001b[0;32m     80\u001b[0m     buffers\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m---> 81\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdump_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\gordo\\Code\\iris-hep\\awkward-20-testing\\.venv\\Lib\\site-packages\\cloudpickle\\cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[0;32m     70\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[0;32m     71\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[0;32m     72\u001b[0m )\n\u001b[1;32m---> 73\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32mc:\\Users\\gordo\\Code\\iris-hep\\awkward-20-testing\\.venv\\Lib\\site-packages\\cloudpickle\\cloudpickle_fast.py:632\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot pickle '_thread.lock' object",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJetPt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJetPt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gordo\\Code\\iris-hep\\awkward-20-testing\\.venv\\Lib\\site-packages\\dask\\base.py:379\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    356\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \n\u001b[0;32m    358\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\gordo\\Code\\iris-hep\\awkward-20-testing\\.venv\\Lib\\site-packages\\dask\\base.py:665\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    662\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 665\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\gordo\\Code\\iris-hep\\awkward-20-testing\\.venv\\Lib\\site-packages\\distributed\\protocol\\serialize.py:379\u001b[0m, in \u001b[0;36mserialize\u001b[1;34m(x, serializers, on_error, context, iterate_collection)\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg, str_x) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mon_error\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m; expected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: ('Could not serialize object of type HighLevelGraph', '<ToPickle: HighLevelGraph with 3 layers.\\n<dask.highlevelgraph.HighLevelGraph object at 0x25c91d93390>\\n 0. sx_fetcher\\n 1. uproot_loader\\n 2. getitem-917c0def67e4544c397316e729412f33\\n>')"
     ]
    }
   ],
   "source": [
    "ar.JetPt[ar.JetPt > 100.0].compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
