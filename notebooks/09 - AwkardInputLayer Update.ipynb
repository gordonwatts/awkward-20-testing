{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When should `AwkwardInputLayer`'s number of partitions be updated?\n",
    "\n",
    "We have an `AwkwardInputLayer` that has 10 partitions - but we don't know that till late in the game. So lets start with a single partition, and then update it when the user calls `.compute()` on the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_awkward as dak\n",
    "import awkward as ak\n",
    "import dask\n",
    "import uproot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Layer\n",
    "\n",
    "The function that will generate the data - it is dead simple as it always returns the same data (but _pretend_!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(i_partition: int) -> ak.Array:\n",
    "    '''Generate the data for a particular partition. There is always one\n",
    "    file per partition.\n",
    "\n",
    "    Use a file that is from an actual SX query, and has \"JetPt' in it.\n",
    "\n",
    "    Args:\n",
    "        block (int): The block number\n",
    "    '''\n",
    "    print(f\"Loading data for partition {i_partition}\")\n",
    "    filename = \"0fc6e51a5ea6dea107c195591d20a1b2-15.26710677._000019.pool.root.1\"\n",
    "    pt = uproot.open(filename)['treeme'].arrays(library='ak')  # type: ignore\n",
    "    return pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can create the input layer. THis is the \"dummy\" version - with only 1 partition when we create, but want to come back during the `compute()` call and update to be 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_layer = \"SX_Query_Result\"\n",
    "\n",
    "input_layer = dak.layers.AwkwardInputLayer(\n",
    "    name=name_layer,\n",
    "    inputs=[0],\n",
    "    io_func=generate_data,\n",
    ")\n",
    "\n",
    "old_mock_input_layer = input_layer.mock\n",
    "def new_mock(s):\n",
    "    print(f\"Mocking {s}\")\n",
    "    return old_mock_input_layer(s)\n",
    "input_layer.mock = new_mock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating the high level array, we need the metadata/type info that describes the Schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>[...]\n",
       "------------------------\n",
       "type: ## * {\n",
       "    JetPt: var * float64\n",
       "}</pre>"
      ],
      "text/plain": [
       "<Array-typetracer [...] type='## * {JetPt: var * float64}'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_array = ak.from_iter([\n",
    "    {\"JetPt\": [1.0, 2.1]},\n",
    "])\n",
    "type_info = dak.core.typetracer_array(sample_array)\n",
    "type_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the high level array that we can then \"query\" against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlg = dask.highlevelgraph.HighLevelGraph.from_collections(name_layer, input_layer)\n",
    "ar = dak.core.new_array_object(hlg, name_layer, meta=type_info, npartitions=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the query and run\n",
    "\n",
    "When we run this, we want it to print out:\n",
    "\n",
    "```text\n",
    "Loading data for partition 0\n",
    "Loading data for partition 1\n",
    "Loading data for partition 2\n",
    "Loading data for partition 3\n",
    "Loading data for partition 4\n",
    "Loading data for partition 5\n",
    "Loading data for partition 6\n",
    "Loading data for partition 7\n",
    "Loading data for partition 8\n",
    "Loading data for partition 9\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for partition 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>[[509, 315],\n",
       " [346, 251, 179],\n",
       " [548, 172, 166],\n",
       " [667, 505],\n",
       " [631, 278, 190],\n",
       " [406, 272, 177, 160],\n",
       " [353, 353],\n",
       " [656, 484, 183, 152],\n",
       " [225, 188],\n",
       " [486, 382],\n",
       " ...,\n",
       " [670, 577, 152],\n",
       " [709, 642, 162],\n",
       " [761, 543, 170],\n",
       " [580, 468, 171, 159],\n",
       " [320, 254, 198],\n",
       " [572, 462, 168, 160],\n",
       " [605, 545, 315],\n",
       " [571, 233, 222, 164],\n",
       " [763, 452, 270]]\n",
       "---------------------------\n",
       "type: 50000 * var * float64</pre>"
      ],
      "text/plain": [
       "<Array [[509, 315], [...], ..., [763, 452, 270]] type='50000 * var * float64'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt = ar.JetPt * 5\n",
    "pt.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.awkward<multiply, npartitions=1>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing it by hand\n",
    "\n",
    "Try changing the `.inputs` by hand (`ar.npartitions` can't be set it seems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.awkward<multiply, npartitions=10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer.inputs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "def update_layer_partitions(coll, n):\n",
    "    hlg_sorted = coll.dask._toposort_layers()\n",
    "    coll._npartitions = n\n",
    "    coll._divisions = (None,)*(n+1)\n",
    "    for dep in hlg_sorted:\n",
    "        layer = coll.dask.layers[dep]\n",
    "        layer._divisions = (None,) * (n+1)\n",
    "        layer._npartitions = n\n",
    "\n",
    "update_layer_partitions(pt, 10)\n",
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for partition 0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "('multiply-d4abc3215384eb2e9efe157ed9f1507a', 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gordo\\Code\\iris-hep\\awkward-20-testing\\.venv\\Lib\\site-packages\\dask\\base.py:379\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    356\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \n\u001b[0;32m    358\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\gordo\\Code\\iris-hep\\awkward-20-testing\\.venv\\Lib\\site-packages\\dask\\base.py:665\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    662\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 665\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\gordo\\Code\\iris-hep\\awkward-20-testing\\.venv\\Lib\\site-packages\\dask\\local.py:305\u001b[0m, in \u001b[0;36mnested_get\u001b[1;34m(ind, coll)\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(nested_get(i, coll) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ind)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcoll\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: ('multiply-d4abc3215384eb2e9efe157ed9f1507a', 1)"
     ]
    }
   ],
   "source": [
    "pt.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
