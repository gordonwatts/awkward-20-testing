{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DASK to ServiceX\n",
    "\n",
    "In this demo we'll take advantage of DASK and ServiceX.\n",
    "\n",
    "## Assumptions:\n",
    "\n",
    "* We don't start anything until we know the number of files that SX will produce\n",
    "* We are ok with some files failing coming out of SX\n",
    "* We are going to do one partition per file\n",
    "* When we start we don't necessarily know all the files produced.\n",
    "\n",
    "## Design Outline\n",
    "\n",
    "* A single `dask` task/layer that has a single output per partition. The output is just a string.\n",
    "* The `AwkwardInputLayer` that has looks at the task input and loads that data from `minio`\n",
    "\n",
    "This version of things will work only for local files - once this works we can move it to a SX prototype.\n",
    "\n",
    "This was written before any code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research\n",
    "\n",
    "A few things to help get started below.\n",
    "\n",
    "### How does the current uproot/dask work?\n",
    "\n",
    "The `from_map` is the base of how `dask_awkward` builds the production. One very nice thing - this is a public interface, so we\n",
    "can rely on it in libraries. \n",
    "\n",
    "```python\n",
    "def from_map(\n",
    "    func: Callable,\n",
    "    *iterables: Iterable,\n",
    "    args: tuple[Any, ...] | None = None,\n",
    "    label: str | None = None,\n",
    "    token: str | None = None,\n",
    "    divisions: tuple[int, ...] | tuple[None, ...] | None = None,\n",
    "    meta: ak.Array | None = None,\n",
    "    **kwargs: Any,\n",
    ") -> Array | tuple[Array, Array]:\n",
    "    \"\"\"Create an Array collection from a custom mapping.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func : Callable\n",
    "        Function used to create each partition.\n",
    "    *iterables : Iterable\n",
    "        Iterable objects to map to each output partition. All\n",
    "        iterables must be the same length. This length determines the\n",
    "        number of partitions in the output collection (only one\n",
    "        element of each iterable will be passed to `func` for each\n",
    "        partition).\n",
    "    args : tuple\n",
    "        Tuple of positional arguments to append after mapped arguments.\n",
    "    label : str, optional\n",
    "        String to use as the function-name label in the output\n",
    "        collection-key names.\n",
    "    token : str, optional\n",
    "        String to use as the \"token\" in the output collection-key names.\n",
    "    divisions : tuple[int, ...] | tuple[None, ...], optional\n",
    "        Partition boundaries (if known).\n",
    "    meta : Array, optional\n",
    "        Collection metadata array, if known (the awkward-array type\n",
    "        tracer)\n",
    "    **kwargs : Any\n",
    "        Keyword arguments passed to `func`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Array\n",
    "        Array collection.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if not callable(func):\n",
    "        raise ValueError(\"`func` argument must be `callable`\")\n",
    "    lengths = set()\n",
    "    iters: list[Iterable] = list(iterables)\n",
    "    for i, iterable in enumerate(iters):\n",
    "        if not isinstance(iterable, Iterable):\n",
    "            raise ValueError(\n",
    "                f\"All elements of `iterables` must be Iterable, got {type(iterable)}\"\n",
    "            )\n",
    "        try:\n",
    "            lengths.add(len(iterable))  # type: ignore\n",
    "        except (AttributeError, TypeError):\n",
    "            iters[i] = list(iterable)\n",
    "            lengths.add(len(iters[i]))  # type: ignore\n",
    "    if len(lengths) == 0:\n",
    "        raise ValueError(\"`from_map` requires at least one Iterable input\")\n",
    "    elif len(lengths) > 1:\n",
    "        raise ValueError(\"All `iterables` must have the same length\")\n",
    "    if lengths == {0}:\n",
    "        raise ValueError(\"All `iterables` must have a non-zero length\")\n",
    "\n",
    "    # Check for `produces_tasks` and `creation_info`\n",
    "    produces_tasks = kwargs.pop(\"produces_tasks\", False)\n",
    "    # creation_info = kwargs.pop(\"creation_info\", None)\n",
    "\n",
    "    if produces_tasks or len(iters) == 1:\n",
    "        if len(iters) > 1:\n",
    "            # Tasks are not detected correctly when they are \"packed\"\n",
    "            # within an outer list/tuple\n",
    "            raise ValueError(\n",
    "                \"Multiple iterables not supported when produces_tasks=True\"\n",
    "            )\n",
    "        inputs = list(iters[0])\n",
    "        packed = False\n",
    "    else:\n",
    "        # Structure inputs such that the tuple of arguments pair each 0th,\n",
    "        # 1st, 2nd, ... elements together; for example:\n",
    "        # from_map(f, [1, 2, 3], [4, 5, 6]) --> [f(1, 4), f(2, 5), f(3, 6)]\n",
    "        inputs = list(zip(*iters))\n",
    "        packed = True\n",
    "\n",
    "    # Define collection name\n",
    "    label = label or funcname(func)\n",
    "    token = token or tokenize(func, iters, meta, **kwargs)\n",
    "    name = f\"{label}-{token}\"\n",
    "\n",
    "    # Define io_func\n",
    "\n",
    "    # FIXME: projection etc.\n",
    "    if packed or args or kwargs:\n",
    "        func = PackedArgCallable(\n",
    "            func,\n",
    "            args=args,\n",
    "            kwargs=kwargs,\n",
    "            packed=packed,\n",
    "        )\n",
    "\n",
    "    # Special `io_func` implementations can implement mocking and optionally\n",
    "    # support buffer projection.\n",
    "    if io_func_implements_mocking(func):\n",
    "        io_func = func\n",
    "        array_meta = cast(ImplementsMocking, func).mock()\n",
    "    # If we know the meta, we can spoof mocking\n",
    "    elif meta is not None:\n",
    "        io_func = IOFunctionWithMocking(meta, func)\n",
    "        array_meta = meta\n",
    "    # Without `meta`, the meta will be computed by executing the graph\n",
    "    else:\n",
    "        io_func = func\n",
    "        array_meta = None\n",
    "\n",
    "    dsk = AwkwardInputLayer(name=name, inputs=inputs, io_func=io_func)\n",
    "\n",
    "    hlg = HighLevelGraph.from_collections(name, dsk)\n",
    "    if divisions is not None:\n",
    "        result = new_array_object(hlg, name, meta=array_meta, divisions=divisions)\n",
    "    else:\n",
    "        result = new_array_object(hlg, name, meta=array_meta, npartitions=len(inputs))\n",
    "\n",
    "    if io_func_implements_report(io_func):\n",
    "        if cast(ImplementsReport, io_func).return_report:\n",
    "            res = result.map_partitions(first, meta=array_meta, output_divisions=1)\n",
    "            rep = result.map_partitions(second, meta=empty_typetracer())\n",
    "            return res, rep\n",
    "\n",
    "    return result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_awkward as dak\n",
    "import awkward as ak\n",
    "import dask\n",
    "import uproot\n",
    "\n",
    "from dask.highlevelgraph import Layer, HighLevelGraph\n",
    "from dask.distributed import Client\n",
    "from typing import AbstractSet\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `uproot.dask` hack way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `awkward.Form` file form\n",
    "\n",
    "We need the form from the schema to prevent us from having to open files that do not yet exist in hour hack. Eventually we'll have to build this from the schema we know exists from the `func_adl` query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecordForm([ListOffsetForm('i64', NumpyForm('float64'))], ['JetPt'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_filename = \"0fc6e51a5ea6dea107c195591d20a1b2-15.26710677._000019.pool.root.1\"\n",
    "with uproot.open(dummy_filename) as file:\n",
    "    file_form = file['treeme'].arrays().layout.form\n",
    "\n",
    "file_form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>[[102, 63.1],\n",
       " [69.3, 50.2, 35.7],\n",
       " [110, 34.5, 33.1],\n",
       " [133, 101],\n",
       " [126, 55.7, 38],\n",
       " [81.1, 54.4, 35.3, 32],\n",
       " [70.6, 70.5],\n",
       " [131, 96.9, 36.6, 30.5],\n",
       " [45.1, 37.6],\n",
       " [97.2, 76.4],\n",
       " ...,\n",
       " [134, 115, 30.4],\n",
       " [142, 128, 32.5],\n",
       " [152, 109, 34.1],\n",
       " [116, 93.6, 34.2, 31.9],\n",
       " [64, 50.8, 39.5],\n",
       " [114, 92.4, 33.6, 32],\n",
       " [121, 109, 63.1],\n",
       " [114, 46.6, 44.4, 32.9],\n",
       " [153, 90.5, 54]]\n",
       "---------------------------\n",
       "type: 50000 * var * float64</pre>"
      ],
      "text/plain": [
       "<Array [[102, 63.1], [...], ..., [153, 90.5, 54]] type='50000 * var * float64'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ar = uproot.dask({dummy_filename: \"treeme\"}, open_files=False, known_base_form=file_form)\n",
    "test_ar.JetPt.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ServiceX Dask Layer\n",
    "\n",
    "Next a `dask` layer that will eventually poll SX for files that are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SXLayer(Layer):\n",
    "    def __init__(self, name, output_names):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.dependencies = dict()\n",
    "        self.tasks = {\n",
    "            out_name: (lambda: self.get_file(out_name),)\n",
    "            for out_name in output_names\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.tasks[key]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.tasks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tasks)\n",
    "\n",
    "    def is_materialized(self):\n",
    "        return False\n",
    "    \n",
    "    def get_output_keys(self) -> AbstractSet[str | bytes | int | float]:\n",
    "        return set(self.tasks.keys())\n",
    "    \n",
    "    def get_file(self, name):\n",
    "        '''Return the info that is needed by uproot to actually open the file'''\n",
    "        print(f\"Returning info for file {name}: {dummy_filename}\")\n",
    "        return (dummy_filename, 'treeme', 0, 1, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture(*args, **kwargs):\n",
    "    print(f\"Captured: {args} {kwargs}\")\n",
    "\n",
    "\n",
    "def sx_open(n_files: int) -> ak.Array:\n",
    "    assert n_files == 1, \"We only know how to do one file for now\"\n",
    "\n",
    "    # Build the high level array.\n",
    "    files = {\n",
    "        f\"sx_partition_{i}.root\": \"treeme\"\n",
    "        for i in range(0, n_files)\n",
    "    }  \n",
    "    ar = uproot.dask(files, open_files=False, known_base_form=file_form)\n",
    "    assert len(ar.__dask_layers__()) == 1\n",
    "    uproot_layer_name = list(ar.__dask_layers__())[0]\n",
    "\n",
    "    # And the task that will furnish the file names for the uproot layer,\n",
    "    # and make uproot layer dependent on the sx layer.\n",
    "    sx_layer = SXLayer(\"sx_fetcher\", files.keys())\n",
    "    ar.__dask_graph__().layers[\"sx_fetcher\"] = sx_layer\n",
    "    ar.__dask_graph__().dependencies[uproot_layer_name].add(\"sx_fetcher\")\n",
    "    ar.__dask_graph__().dependencies[\"sx_fetcher\"] = set()\n",
    "\n",
    "    # Next, hook up the argument to the uproot functions to the output from the SX layer.\n",
    "    uproot_layer = ar.__dask_graph__().layers[uproot_layer_name]\n",
    "    uproot_layer_outputs = list(uproot_layer.keys())\n",
    "    assert len(uproot_layer_outputs) == n_files\n",
    "    for layer_out, layer_arg in zip(uproot_layer_outputs, files.keys()):\n",
    "        print(f\"Hooking up {layer_out} to {layer_arg}\")\n",
    "        uproot_layer._dict[layer_out] = (capture, f\"{layer_arg}-bogus\")\n",
    "        # uproot_layer._dict[layer_out] = (uproot_layer[layer_out][0], f\"{layer_arg}-bogus\")\n",
    "        print (f\"  {uproot_layer[layer_out]}\")\n",
    "    print(uproot_layer)\n",
    "\n",
    "    return ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-24 15:29:35,298 - distributed.scheduler - WARNING - Worker failed to heartbeat within 300 seconds. Closing: <WorkerState 'tcp://127.0.0.1:64153', name: 2, status: running, memory: 0, processing: 0>\n",
      "2024-02-24 15:29:35,326 - distributed.scheduler - WARNING - Worker failed to heartbeat within 300 seconds. Closing: <WorkerState 'tcp://127.0.0.1:64162', name: 1, status: running, memory: 0, processing: 0>\n",
      "2024-02-24 15:29:35,331 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:64153'.\n",
      "2024-02-24 15:29:35,336 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:64162'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hooking up ('from-uproot-8d1ae5c61dd92ce9355fa2e2a5fc2bc5', 0) to sx_partition_0.root\n",
      "  (<function capture at 0x0000025441B67240>, 'sx_partition_0.root-bogus')\n",
      "AwkwardInputLayer<from-uproot-8d1ae5c61dd92ce9355fa2e2a5fc2bc5>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div>\n",
       "        <div style=\"width: 52px; height: 52px; position: absolute;\">\n",
       "            <svg width=\"76\" height=\"71\" viewBox=\"0 0 76 71\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "                <circle cx=\"61.5\" cy=\"36.5\" r=\"13.5\" style=\"stroke: var(--jp-ui-font-color2, #1D1D1D); fill: var(--jp-layout-color1, #F2F2F2);\" stroke-width=\"2\"/>\n",
       "                <circle cx=\"14.5\" cy=\"14.5\" r=\"13.5\" style=\"stroke: var(--jp-ui-font-color2, #1D1D1D); fill: var(--jp-layout-color1, #F2F2F2);\" stroke-width=\"2\"/>\n",
       "                <circle cx=\"14.5\" cy=\"56.5\" r=\"13.5\" style=\"stroke: var(--jp-ui-font-color2, #1D1D1D); fill: var(--jp-layout-color1, #F2F2F2);\" stroke-width=\"2\"/>\n",
       "                <path d=\"M28 16L30.5 16C33.2614 16 35.5 18.2386 35.5 21L35.5 32.0001C35.5 34.7615 37.7386 37.0001 40.5 37.0001L43 37.0001\" style=\"stroke: var(--jp-ui-font-color2, #1D1D1D);\" stroke-width=\"1.5\"/>\n",
       "                <path d=\"M40.5 37L40.5 37.75L40.5 37.75L40.5 37ZM35.5 42L36.25 42L35.5 42ZM35.5 52L34.75 52L35.5 52ZM30.5 57L30.5 57.75L30.5 57ZM41.5001 36.25L40.5 36.25L40.5 37.75L41.5001 37.75L41.5001 36.25ZM34.75 42L34.75 52L36.25 52L36.25 42L34.75 42ZM30.5 56.25L28.0001 56.25L28.0001 57.75L30.5 57.75L30.5 56.25ZM34.75 52C34.75 54.3472 32.8472 56.25 30.5 56.25L30.5 57.75C33.6756 57.75 36.25 55.1756 36.25 52L34.75 52ZM40.5 36.25C37.3244 36.25 34.75 38.8243 34.75 42L36.25 42C36.25 39.6528 38.1528 37.75 40.5 37.75L40.5 36.25Z\" style=\"fill: var(--jp-ui-font-color2, #1D1D1D);\"/>\n",
       "                <circle cx=\"28\" cy=\"16\" r=\"2.25\" fill=\"#E5E5E5\" style=\"stroke: var(--jp-ui-font-color2, #1D1D1D);\" stroke-width=\"1.5\"/>\n",
       "                <circle cx=\"28\" cy=\"57\" r=\"2.25\" fill=\"#E5E5E5\" style=\"stroke: var(--jp-ui-font-color2, #1D1D1D);\" stroke-width=\"1.5\"/>\n",
       "                <path d=\"M45.25 36.567C45.5833 36.7594 45.5833 37.2406 45.25 37.433L42.25 39.1651C41.9167 39.3575 41.5 39.117 41.5 38.7321V35.2679C41.5 34.883 41.9167 34.6425 42.25 34.8349L45.25 36.567Z\" style=\"fill: var(--jp-ui-font-color2, #1D1D1D);\"/>\n",
       "            </svg>\n",
       "        </div>\n",
       "        <div style=\"margin-left: 64px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">HighLevelGraph</h3>\n",
       "            <p style=\"color: var(--jp-ui-font-color2, #5D5851); margin-bottom:0px;\">\n",
       "                HighLevelGraph with 2 layers and 2 keys from all layers.\n",
       "            </p>\n",
       "            \n",
       "            <div style=\"\">\n",
       "    <svg width=\"24\" height=\"24\" viewBox=\"0 0 32 32\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"position: absolute;\">\n",
       "        \n",
       "        <circle cx=\"16\" cy=\"16\" r=\"14\" style=\"stroke: var(--jp-ui-font-color2, #1D1D1D); fill: var(--jp-layout-color1, #F2F2F2);\" stroke-width=\"2\" />\n",
       "        \n",
       "    </svg>\n",
       "\n",
       "    <details style=\"margin-left: 32px;\">\n",
       "        <summary style=\"margin-bottom: 10px; margin-top: 10px;\">\n",
       "            <h4 style=\"display: inline;\">Layer1: sx_fetcher</h4>\n",
       "        </summary>\n",
       "        <p style=\"color: var(--jp-ui-font-color2, #5D5851); margin: -0.25em 0px 0px 0px;\">\n",
       "            sx_fetcher\n",
       "        </p>\n",
       "\n",
       "        <table>\n",
       "        <tr>\n",
       "            <td>\n",
       "                <table>\n",
       "                \n",
       "                    <tr>\n",
       "                        <th style=\"text-align: left; width: 150px;\">layer_type</th>\n",
       "                        <td style=\"text-align: left;\">SXLayer</td>\n",
       "                    </tr>\n",
       "                \n",
       "                    <tr>\n",
       "                        <th style=\"text-align: left; width: 150px;\">is_materialized</th>\n",
       "                        <td style=\"text-align: left;\">False</td>\n",
       "                    </tr>\n",
       "                \n",
       "                    <tr>\n",
       "                        <th style=\"text-align: left; width: 150px;\">number of outputs</th>\n",
       "                        <td style=\"text-align: left;\">1</td>\n",
       "                    </tr>\n",
       "                \n",
       "                \n",
       "                </table>\n",
       "            </td>\n",
       "            <td>\n",
       "                \n",
       "            </td>\n",
       "        </tr>\n",
       "        </table>\n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "            \n",
       "            <div style=\"\">\n",
       "    <svg width=\"24\" height=\"24\" viewBox=\"0 0 32 32\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"position: absolute;\">\n",
       "        \n",
       "        <circle cx=\"16\" cy=\"16\" r=\"14\" fill=\"#8F8F8F\" style=\"stroke: var(--jp-ui-font-color2, #1D1D1D);\" stroke-width=\"2\"/>\n",
       "        \n",
       "    </svg>\n",
       "\n",
       "    <details style=\"margin-left: 32px;\">\n",
       "        <summary style=\"margin-bottom: 10px; margin-top: 10px;\">\n",
       "            <h4 style=\"display: inline;\">Layer2: from-uproot</h4>\n",
       "        </summary>\n",
       "        <p style=\"color: var(--jp-ui-font-color2, #5D5851); margin: -0.25em 0px 0px 0px;\">\n",
       "            from-uproot-8d1ae5c61dd92ce9355fa2e2a5fc2bc5\n",
       "        </p>\n",
       "\n",
       "        <table>\n",
       "        <tr>\n",
       "            <td>\n",
       "                <table>\n",
       "                \n",
       "                    <tr>\n",
       "                        <th style=\"text-align: left; width: 150px;\">layer_type</th>\n",
       "                        <td style=\"text-align: left;\">AwkwardInputLayer</td>\n",
       "                    </tr>\n",
       "                \n",
       "                    <tr>\n",
       "                        <th style=\"text-align: left; width: 150px;\">is_materialized</th>\n",
       "                        <td style=\"text-align: left;\">True</td>\n",
       "                    </tr>\n",
       "                \n",
       "                    <tr>\n",
       "                        <th style=\"text-align: left; width: 150px;\">number of outputs</th>\n",
       "                        <td style=\"text-align: left;\">1</td>\n",
       "                    </tr>\n",
       "                \n",
       "                \n",
       "                    \n",
       "                        <tr>\n",
       "                            <th style=\"text-align: left; width: 150px;\"> depends on </th>\n",
       "                            <td style=\"text-align: left;\">sx_fetcher</td>\n",
       "                        </tr>\n",
       "                    \n",
       "                \n",
       "                </table>\n",
       "            </td>\n",
       "            <td>\n",
       "                \n",
       "            </td>\n",
       "        </tr>\n",
       "        </table>\n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "            \n",
       "        </div>\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "HighLevelGraph with 2 layers.\n",
       "<dask.highlevelgraph.HighLevelGraph object at 0x25442d75850>\n",
       " 0. sx_fetcher\n",
       " 1. from-uproot-8d1ae5c61dd92ce9355fa2e2a5fc2bc5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sx_open(1)\n",
    "# uproot_key_name = 'from-uproot-8d1ae5c61dd92ce9355fa2e2a5fc2bc5'\n",
    "# key0 = list(a.__dask_graph__().layers[uproot_key_name].keys())[0]\n",
    "# print(key0)\n",
    "# a.__dask_graph__().layers[uproot_key_name][key0][1]\n",
    "# a.__dask_graph__().layers[uproot_key_name][key0][0]\n",
    "a.__dask_graph__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.JetPt.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be running into trouble because we can't really use task outputs as inputs. So we'll need to talk to someone further about that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blockwise Approach\n",
    "\n",
    "Could we start a blockwise approach on its own?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SXLayerBW(Layer):\n",
    "    '''Outputs are just the names of the files that we want to open downstream with uproot'''\n",
    "    def __init__(self, name, n_files):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.dependencies = dict()\n",
    "        self.tasks = {\n",
    "            \"output_0\": (lambda: self.get_file(f\"output_0\"),)\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.tasks[key]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.tasks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tasks)\n",
    "\n",
    "    def is_materialized(self):\n",
    "        return False\n",
    "    \n",
    "    def get_output_keys(self) -> AbstractSet[str | bytes | int | float]:\n",
    "        return set(self.tasks.keys())\n",
    "    \n",
    "    def get_file(self, name):\n",
    "        '''Return the info that is needed by uproot to actually open the file'''\n",
    "        print(f\"Returning info for file {name}: {dummy_filename}\")\n",
    "        return (dummy_filename, 'treeme', 0, 1, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the layer that will load files from the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class URLoaderLayer(Layer):\n",
    "    def __init__(self, name, sx_layer_name, output_name, n_files):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.dependencies = {name: sx_layer_name}\n",
    "        self.tasks = {\n",
    "            (name, i): (lambda f_name: self.get_data(f_name), f'output_{i}')\n",
    "            for i in range(n_files)\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.tasks[key]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.tasks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tasks)\n",
    "\n",
    "    def is_materialized(self):\n",
    "        return False\n",
    "    \n",
    "    def get_output_keys(self) -> AbstractSet[str | bytes | int | float]:\n",
    "        return set(self.tasks.keys())\n",
    "    \n",
    "    def get_data(self, name):\n",
    "        '''Return the info that is needed by uproot to actually open the file'''\n",
    "        print(f\"Returning info for file {name}: {dummy_filename}\")\n",
    "        with uproot.open(dummy_file) as file:\n",
    "            return file['treeme'].arrays()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok - lets build up the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx_layer = SXLayerBW(\"sx_fetcher\", 1)\n",
    "\n",
    "\n",
    "# def sx_open(n_files: int) -> ak.Array:\n",
    "#     assert n_files == 1, \"We only know how to do one file for now\"\n",
    "\n",
    "#     # Build the high level array.\n",
    "#     files = {\n",
    "#         f\"sx_partition_{i}.root\": \"treeme\"\n",
    "#         for i in range(0, n_files)\n",
    "#     }  \n",
    "#     ar = uproot.dask(files, open_files=False, known_base_form=file_form)\n",
    "#     assert len(ar.__dask_layers__()) == 1\n",
    "#     uproot_layer_name = list(ar.__dask_layers__())[0]\n",
    "\n",
    "#     # And the task that will furnish the file names for the uproot layer,\n",
    "#     # and make uproot layer dependent on the sx layer.\n",
    "#     sx_layer = SXLayer(\"sx_fetcher\", files.keys())\n",
    "#     ar.__dask_graph__().layers[\"sx_fetcher\"] = sx_layer\n",
    "#     ar.__dask_graph__().dependencies[uproot_layer_name].add(\"sx_fetcher\")\n",
    "#     ar.__dask_graph__().dependencies[\"sx_fetcher\"] = set()\n",
    "\n",
    "#     # Next, hook up the argument to the uproot functions to the output from the SX layer.\n",
    "#     uproot_layer = ar.__dask_graph__().layers[uproot_layer_name]\n",
    "#     uproot_layer_outputs = list(uproot_layer.keys())\n",
    "#     assert len(uproot_layer_outputs) == n_files\n",
    "#     for layer_out, layer_arg in zip(uproot_layer_outputs, files.keys()):\n",
    "#         print(f\"Hooking up {layer_out} to {layer_arg}\")\n",
    "#         uproot_layer._dict[layer_out] = (capture, f\"{layer_arg}-bogus\")\n",
    "#         # uproot_layer._dict[layer_out] = (uproot_layer[layer_out][0], f\"{layer_arg}-bogus\")\n",
    "#         print (f\"  {uproot_layer[layer_out]}\")\n",
    "#     print(uproot_layer)\n",
    "\n",
    "#     return ar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SX Support\n",
    "\n",
    "The code below belongs in the `sx-awk` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the ServiceX layer. It is responsible for all communication with ServiceX, and finding the files (and URL's) from `minio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Set\n",
    "from typing import AbstractSet, Any, Dict, Iterator, KeysView, List\n",
    "import logging\n",
    "\n",
    "class SXLayer(Layer):\n",
    "    def __init__(self, sx_query_guid):\n",
    "        super().__init__()\n",
    "        self._query_guid = sx_query_guid\n",
    "\n",
    "        # Create a task that will be executed when the layer is computed,\n",
    "        # and will fetch the list of files from SX.\n",
    "        k = f\"SX-query-{self._query_guid}\"\n",
    "        self._tasks: Dict[str, Any] = {k: dask.delayed(self._fetch_files, name=k)}\n",
    "\n",
    "    def _fetch_files(self) -> str:\n",
    "        # This is where the actual fetching of the files from SX would happen.\n",
    "        # For now, just return a list of files.\n",
    "        logging.warn(\"Returning a file\")\n",
    "        return \"0fc6e51a5ea6dea107c195591d20a1b2-15.26710677._000019.pool.root.1\"\n",
    "\n",
    "    def __getitem__(self, __key) -> Any:\n",
    "        return self._tasks[str(__key)]\n",
    "    \n",
    "    def keys(self):\n",
    "        return self._tasks.keys()\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self._tasks)\n",
    "    \n",
    "    def get_output_keys(self) -> AbstractSet[str]:\n",
    "        return {f\"SX-query-{self._query_guid}\"}\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._tasks)\n",
    "    \n",
    "    def is_materialized(self) -> bool:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = SXLayer(\"182382781\")\n",
    "hlg = HighLevelGraph(\n",
    "    layers={\"l1\": l},\n",
    "    dependencies={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = client.compute(hlg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `uproot.dask` way\n",
    "\n",
    "This is a very simple call - here for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"0fc6e51a5ea6dea107c195591d20a1b2-15.26710677._000019.pool.root.1\"\n",
    "ar = uproot.dask({filename: \"treeme\"}, open_files=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = ar.JetPt * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lets look at the layers/etc. for this for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = pt.__dask_graph__()\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the input layer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_uproot_key = [k for k in graph.layers.keys() if k.startswith(\"from\")][0]\n",
    "print(f\"From uproot key: {from_uproot_key}\")\n",
    "from_uproot_first_output = list(graph.layers[from_uproot_key].keys())[0]\n",
    "print(f\"From uproot first output: {from_uproot_first_output}\")\n",
    "print(f\"The function that gets executed and the arguments: {graph.layers[from_uproot_key][from_uproot_first_output]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the `('0fc6e51a5ea6dea107c195591d20a1b2-15.26710677._000019.pool.root.1', 'treeme', 0, 1, False)` to be the argument/output from a previous run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Demo\n",
    "\n",
    "Used AI to come up with this (minus a few syntax errors and missing the `Client.get`). This shows how to build everything from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Set\n",
    "import typing\n",
    "\n",
    "\n",
    "class CustomLayer(Layer):\n",
    "    def __init__(self, name, dependencies, tasks):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.dependencies = dependencies\n",
    "        self.tasks = tasks\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.tasks[key]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.tasks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tasks)\n",
    "\n",
    "    def is_materialized(self):\n",
    "        return False\n",
    "    \n",
    "    def get_output_keys(self) -> AbstractSet[str | bytes | int | float]:\n",
    "        return set(self.tasks.keys())\n",
    "\n",
    "# Define the tasks for each layer\n",
    "tasks1 = {\"output1\": (lambda: \"result1\", ), \"output2\": (lambda: \"result2\", )}\n",
    "tasks2 = {\"output3\": (lambda x: x + \" processed\", \"output1\"), \"output4\": (lambda x: x + \" processed\", \"output2\")}\n",
    "\n",
    "# Create the layers\n",
    "layer1 = CustomLayer(\"layer1\", [], tasks1)\n",
    "layer2 = CustomLayer(\"layer2\", [\"layer1\"], tasks2)\n",
    "\n",
    "# Create the high level graph\n",
    "hlg = HighLevelGraph(\n",
    "    layers={layer1.name: layer1, layer2.name: layer2},\n",
    "    dependencies={layer2.name: {layer1.name}, layer1.name: set()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get(hlg, [\"output4\", \"output3\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
